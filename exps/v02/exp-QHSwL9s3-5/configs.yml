_id:
  project: flame
  version: null
db:
  query:
    dat-raw: /data/raw/flame/proc/v02/{sid}/dat.hdf5
    hst-raw: /data/raw/flame/proc/v02/{sid}/hst.hdf5
    lbl-raw: /data/raw/flame/proc/v02/{sid}/lbl.hdf5
    dst-raw: /data/raw/flame/proc/v02/{sid}/dst.hdf5
  sform:
    dat-raw: '{root}/proc/v02/{sid}/dat.hdf5'
    hst-raw: '{root}/proc/v02/{sid}/hst.hdf5'
    lbl-raw: '{root}/proc/v02/{sid}/lbl.hdf5'
    dst-raw: '{root}/proc/v02/{sid}/dst.hdf5'
  fdefs: []
  setup:
    prefix: db-v02
    ignore_missing: false
    create_symlinks: false
    create_raw: false
    create_header_from_csv: '{root}/prep/csvs/meta.csv'
    create_groups: null
    create_stats:
    - name: [shape, vsize, mu, sd, min, max]
      cols: [dat, hst]
      mask: true
    - name: [max, vol]
      cols: [lbl]
    create_cohorts:
    - name: [percentile]
      cols: [lbl-vol]
      bins: 5
    create_valid:
      folds: 5
      group: null
    create_model:
      subsample: false
      max_shape: [1, 128, 128]
      max_ratio: 2
      ignore_vsize: true
client:
  batch:
    fold: -1
    size: 4
    sampling:
      cohort-cls0: 0.1
      cohort-cls1: 0.1
      cohort-cls2: 0.1
      cohort-cls3: 0.1
      cohort-cls4: 0.6
  specs:
    in_cache: false
    in_memory: true
    xs:
      x:
        dtype: float32
        loads: hst-raw
        shape: [1, 1200, 1200, 11]
      y:
        dtype: float32
        loads: lbl-raw
        shape: [1, 1200, 1200, 1]
    load_kwargs:
      verbose: false
xforms:
  patch_size: [1, 256, 256]
  sampling: null
  sampling_binarize: false
  norm_lower: null
  norm_upper: null
  rand_aff_2d: true
  rand_aff_sca: [0.8, 1.2]
  rand_aff_rot: [-0.5, 0.5]
  rand_aff_sca_rigid: true
  rand_shift: [-0.1, 0.1]
  rand_scale: [0.9, 1.1]
  y: [y]
  m: null
  classes:
    y: 3
  mapped: null
  n_samples: 2
  batch_size: 4
mapped:
  y:
  - key: y
    old: {eq: [1, 2]}
    new: 1
  w:
  - key: y
    old: {gte: 0}
    new: 1
layers:
  norm: layer_norm
  func: leaky_relu
blocks: {}
losses: {}
models:
  kwargs: {}
  backbone:
    name: create_backbone
    configs:
      models:
        pattern:
          name: create_pattern
          names: [encoder, decoder, head]
  head:
    name: create_head
    base: head
    sources:
      strategy: deep
      previous: decoder
    filters: [1]
  training:
    name: create_training
    configs:
      models:
        losses:
          functions:
          - name: foc
            gamma: 2.0
            alpha: 1.0
            y_true: y
            y_pred: head
            w_samp: w
            w_loss: 1.0
          - name: sft
            y_true: y
            y_pred: head
            w_samp: w
            w_loss: 1.0
        metric:
          functions:
          - name: dsc
            y_true: y
            y_pred: head
            w_samp: w
  optimizer:
    name: create_optimizer
    method: Adam
    learning_rate: 0.001
  lr_decay:
    name: create_lr_decay
    decay: 0.99
  callbacks:
    name: create_callbacks
    names: [lr_decay]
params:
  steps: 40000
  steps_per_epoch: 100
  validation_freq: 5
  validation_steps: 100
  save_freq: 10
  save_best_only: []
  save_best_mode: max
  save_paths:
    training: '{output_dir}/{save_format}/{base}'
    backbone: '{output_dir}/{save_format}/{base}/{base}_{offset:03d}'
search:
  file: '{root}/exps/v02/jmodels/csvs/hyper.csv'
output:
  stats:
  - metric: [dsc, h95, 4x4]
    y_true: y
    y_pred: head-0
    w_samp: w
    thresh: [0.01, 0.02, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.97, 0.98, 0.99]
    models:
      epochs: [-1]
      graphs: [backbone]
  cohort:
    suffix: last
    column:
      valid: -1
  merged: []
